---
id: nvidia-gpu
title: Supporting the NVIDIA GPU
description: It provides the method how to collect GPU data of the Kubernetes agent.
keywords:
  - Kubernetes
  - Kubernetes Monitoring
  - Support Environment
  - GPU
---

## How to collect GPU metrics of the WhaTap Kubernetes agent

The WhaTap Kubernetes node agent collects and monitors performance metrics of the NVIDIA GPU by using the Data Center GPU Manager (DCGM) Exporter. The process is structured using the Sidecar pattern.

- **Sidecar pattern**

  The DCGM Exporter is set as a secondary container that runs within the same Pod together with the main application container. This Sidecar pattern helps the DCGM Exporter efficiently collect the GPU status information.

- **DCGM Exporter** 

  The `dcgm-exporter` container collects GPU status and performance metrics via NVIDIA's Data Center GPU Manager (DCGM). 

- **Metric collection and transmission** 

  The `whatap-node-agent` container requests and collects GPU metrics via the HTTP endpoint of `dcgm-exporter`. 

  :::note 

  The HTTP endpoint of `dcgm-exporter` usually uses the port 9400.

  :::

- **MIG environment support** 

  WhaTap Kubernetes supports the **MIG (Multi-Instance GPU)** feature in the NVIDIA GPU cluster environment, providing **granular monitoring from physical GPU units to MIG instances**. 

  In MIG mode, you can partition a single physical GPU into multiple logical instances to isolate workloads and allocate resources. WhaTap supports collection of accurate metrics and visualization even in the environment.

## Collection metrics

The following lists the key GPU metrics collected by the DCGM Exporter: 

<div style={{ overflowX: 'auto' }}>
<table style={{ whiteSpace: 'nowrap' }}>
  <thead>
    <tr>
      <th>Title</th>
      <th>Metrics</th>
      <th>Unit</th>
      <th>Label</th>
      <th>Description</th>
      <th>Code</th>
      <th>Category</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>`DCGM_FI_PROF_GR_ENGINE_ACTIVE for Devices`</td>
      <td>`DCGM_FI_PROF_GR_ENGINE_ACTIVE`</td>
      <td>%</td>
      <td>`GPU ${gpu} [Node: ${onodeName}]` <br /> e.g) `GPU 5 [Node: ip-10-143-180]`</td>
      <td>The DCGM_FI_PROF_GR_ENGINE_ACTIVE metric collected from DCGM is adjusted by the weight (Compute Instance ratio) in MIG mode and aggregated per GPU physical device unit.</td>
      <td><a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-ids.html">1001</a></td>
      <td>DCP</td>
    </tr>
    <tr>
      <td>`DCGM_FI_PROF_GR_ENGINE_ACTIVE for MIG Instance`</td>
      <td>`DCGM_FI_PROF_GR_ENGINE_ACTIVE`</td>
      <td>%</td>
      <td>`GPU ${gpu} / ${GPU_I_ID}(${GPU_I_PROFILE}) / ${DCGM_FI_DEV_UUID} [${onodeName}]` <br /> e.g) `GPU 5 / 11(2g.10gb) / MIG-1234-5678-c [ip-10-143-180]`</td>
      <td>The DCGM_FI_PROF_GR_ENGINE_ACTIVE metric collected from DCGM is adjusted by the weight (Compute Instance ratio) in MIG mode and aggregated per MIG instance unit.</td>
      <td><a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-ids.html">1001</a></td>
      <td>DCP</td>
    </tr>
    <tr>
      <td>`DCGM_FI_PROF_GR_ENGINE_ACTIVE Node-Level Average`</td>
      <td>`DCGM_FI_PROF_GR_ENGINE_ACTIVE`</td>
      <td>%</td>
      <td>`${onodeName}`<br /> e.g) `ip-10-143-180`</td>
      <td>The DCGM_FI_PROF_GR_ENGINE_ACTIVE metric collected from DCGM is adjusted by the weight (Compute Instance ratio) in MIG mode and aggregated per node unit.</td>
      <td><a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-ids.html">1001</a></td>
      <td>DCP</td>
    </tr>
    <tr>
      <td>`DCGM_FI_PROF_GR_ENGINE_ACTIVE Pod-Level`</td>
      <td>`DCGM_FI_PROF_GR_ENGINE_ACTIVE`</td>
      <td>%</td>
      <td>`namespace=${namespace}, pod=${pod}, node=${onodeName}` <br />
      `[GPU ${gpu} / ${GPU_I_ID}(${GPU_I_PROFILE}) / ${DCGM_FI_DEV_UUID}]`<br /> <br />
      e.g) <br /> `namespace=whatap, pod=gpu-mig-5-a, node=ip-10-143-180 [GPU 5 / 11(2g.10gb) / MIG-1234-5678-c]`<br />
      `namespace=whatap, pod=gpu-no-mig-2, node=ip-10-143-140 [GPU 2 / null(null) / GPU-bcde-fghi]`</td>
      <td>The DCGM_FI_PROF_GR_ENGINE_ACTIVE metric collected from DCGM is adjusted by the weight (Compute Instance ratio) in MIG mode and aggregated per Pod unit.</td>
      <td><a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-ids.html">1001</a></td>
      <td>DCP</td>
    </tr>
    <tr>
      <td>`CGM_FI_PROF_DRAM_ACTIVE for Devices`</td>
      <td>`DCGM_FI_PROF_DRAM_ACTIVE`</td>
      <td>%</td>
      <td>`GPU ${gpu} [Node: ${onodeName}]`<br /> e.g) `GPU 5 [Node: ip-10-143-180]`</td>
      <td>The DCGM_FI_PROF_DRAM_ACTIVE metric collected from DCGM is adjusted by the weight (Compute Instance ratio) in MIG mode and aggregated per GPU physical device unit.</td>
      <td><a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-ids.html">1005</a></td>
      <td>DCP</td>
    </tr>
    <tr>
      <td>`DCGM_FI_PROF_DRAM_ACTIVE for MIG Instance`</td>
      <td>`DCGM_FI_PROF_DRAM_ACTIVE`</td>
      <td>%</td>
      <td>`GPU ${gpu} / ${GPU_I_ID}(${GPU_I_PROFILE}) / ${DCGM_FI_DEV_UUID} [${onodeName}]` <br /> 
      e.g) `GPU 5 / 11(2g.10gb) / MIG-1234-5678-c [ip-10-143-180]`</td>
      <td>The DCGM_FI_PROF_DRAM_ACTIVE metric collected from DCGM is adjusted by the weight (Compute Instance ratio) in MIG mode and aggregated per MIG instance unit.</td>
      <td><a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-ids.html">1005</a></td>
      <td>DCP</td>
    </tr>
    <tr>
      <td>`DCGM_FI_PROF_DRAM_ACTIVE Node-Level Average`</td>
      <td>`DCGM_FI_PROF_DRAM_ACTIVE`</td>
      <td>%</td>
      <td>`${onodeName}` <br /> e.g) `ip-10-143-180`</td>
      <td>The DCGM_FI_PROF_DRAM_ACTIVE metric collected from DCGM is adjusted by the weight (Compute Instance ratio) in MIG mode and aggregated per node unit.</td>
      <td><a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-ids.html">1005</a></td>
      <td>DCP</td>
    </tr>
    <tr>
      <td>`DCGM_FI_PROF_DRAM_ACTIVE Pod-Level`</td>
      <td>`DCGM_FI_PROF_DRAM_ACTIVE`</td>
      <td>%</td>
      <td>`namespace=${namespace}, pod=${pod}, node=${onodeName}` <br /> 
      `[GPU ${gpu} / ${GPU_I_ID}(${GPU_I_PROFILE}) / ${DCGM_FI_DEV_UUID}]` <br /> <br />
      e.g) <br />
      `namespace=whatap, pod=gpu-mig-5-a, node=ip-10-143-180 [GPU 5 / 11(2g.10gb) / MIG-1234-5678-c]`<br />
      `namespace=whatap, pod=gpu-no-mig-2, node=ip-10-143-140 [GPU 2 / null(null) / GPU-bcde-fghi]`</td>
      <td>The DCGM_FI_PROF_DRAM_ACTIVE metric collected from DCGM is adjusted by the weight (Compute Instance ratio) in MIG mode and aggregated per Pod unit.</td>
      <td><a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-ids.html">1005</a></td>
      <td>DCP</td>
    </tr>
    <tr>
      <td>`DCGM_FI_PROF_SM_ACTIVE for Devices`</td>
      <td>`DCGM_FI_PROF_SM_ACTIVE`</td>
      <td>%</td>
      <td>`GPU ${gpu} [Node: ${onodeName}]` <br /> e.g) `GPU 5 [Node: ip-10-143-180]`</td>
      <td>The DCGM_FI_PROF_SM_ACTIVE metric collected from DCGM is adjusted by the weight (Compute Instance ratio) in MIG mode and aggregated per GPU physical device unit.</td>
      <td><a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-ids.html">1002</a></td>
      <td>DCP</td>
    </tr>
    <tr>
      <td>`DCGM_FI_PROF_SM_ACTIVE for MIG Instance`</td>
      <td>`DCGM_FI_PROF_SM_ACTIVE`</td>
      <td>%</td>
      <td>`GPU ${gpu} / ${GPU_I_ID}(${GPU_I_PROFILE}) / ${DCGM_FI_DEV_UUID} [${onodeName}]` <br /> 
      e.g) `GPU 5 / 11(2g.10gb) / MIG-1234-5678-c [ip-10-143-180]`</td>
      <td>The DCGM_FI_PROF_SM_ACTIVE metric collected from DCGM is adjusted by the weight (Compute Instance ratio) in MIG mode and aggregated per MIG instance unit.</td>
      <td><a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-ids.html">1002</a></td>
      <td>DCP</td>
    </tr>
    <tr>
      <td>`DCGM_FI_PROF_SM_ACTIVE Node-Level Average`</td>
      <td>`DCGM_FI_PROF_SM_ACTIVE`</td>
      <td>%</td>
      <td>`${onodeName}` <br /> e.g) `ip-10-143-180`</td>
      <td>The DCGM_FI_PROF_SM_ACTIVE metric collected from DCGM is adjusted by the weight (Compute Instance ratio) in MIG mode and aggregated per node unit.</td>
      <td><a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-ids.html">1002</a></td>
      <td>DCP</td>
    </tr>
    <tr>
      <td>`DCGM_FI_PROF_SM_ACTIVE Pod-Level`</td>
      <td>`DCGM_FI_PROF_SM_ACTIVE`</td>
      <td>%</td>
      <td>`namespace=${namespace}, pod=${pod}, node=${onodeName}` <br /> 
      `[GPU ${gpu} / ${GPU_I_ID}(${GPU_I_PROFILE}) / ${DCGM_FI_DEV_UUID}]` <br /> <br />
      e.g) <br />
      `namespace=whatap, pod=gpu-mig-5-a, node=ip-10-143-180 [GPU 5 / 11(2g.10gb) / MIG-1234-5678-c]` <br /> 
      `namespace=whatap, pod=gpu-no-mig-2, node=ip-10-143-140 [GPU 2 / null(null) / GPU-bcde-fghi]`</td>
      <td>The DCGM_FI_PROF_SM_ACTIVE metric collected from DCGM is adjusted by the weight (Compute Instance ratio) in MIG mode and aggregated per Pod unit.</td>
      <td><a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-ids.html">1002</a></td>
      <td>DCP</td>
    </tr>
    <tr>
      <td>`DCGM_FI_PROF_PIPE_TENSOR_ACTIVE for Devices`</td>
      <td>`DCGM_FI_PROF_PIPE_TENSOR_ACTIVE`</td>
      <td>%</td>
      <td>`GPU ${gpu} [Node: ${onodeName}]` <br /> e.g) `GPU 5 [Node: ip-10-143-180]`</td>
      <td>The DCGM_FI_PROF_PIPE_TENSOR_ACTIVE metric collected from DCGM is adjusted by the weight (Compute Instance ratio) in MIG mode and aggregated per GPU physical device unit.</td>
      <td><a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-ids.html">1004</a></td>
      <td>DCP</td>
    </tr>
    <tr>
      <td>`DCGM_FI_PROF_PIPE_TENSOR_ACTIVE for MIG Instance`</td>
      <td>`DCGM_FI_PROF_PIPE_TENSOR_ACTIVE`</td>
      <td>%</td>
      <td>`GPU ${gpu} / ${GPU_I_ID}(${GPU_I_PROFILE}) / ${DCGM_FI_DEV_UUID} [${onodeName}]` <br />
      e.g) `GPU 5 / 11(2g.10gb) / MIG-1234-5678-c [ip-10-143-180]`</td>
      <td>The DCGM_FI_PROF_PIPE_TENSOR_ACTIVE metric collected from DCGM is adjusted by the weight (Compute Instance ratio) in MIG mode and aggregated per MIG instance unit.</td>
      <td><a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-ids.html">1004</a></td>
      <td>DCP</td>
    </tr>
    <tr>
      <td>`DCGM_FI_PROF_PIPE_TENSOR_ACTIVE Node-Level Average`</td>
      <td>`DCGM_FI_PROF_PIPE_TENSOR_ACTIVE`</td>
      <td>%</td>
      <td>`${onodeName}` <br /> e.g) `ip-10-143-180`</td>
      <td>The DCGM_FI_PROF_PIPE_TENSOR_ACTIVE metric collected from DCGM is adjusted by the weight (Compute Instance ratio) in MIG mode and aggregated per node unit.</td>
      <td><a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-ids.html">1004</a></td>
      <td>DCP</td>
    </tr>
    <tr>
      <td>`DCGM_FI_PROF_PIPE_TENSOR_ACTIVE Pod-Level`</td>
      <td>`DCGM_FI_PROF_PIPE_TENSOR_ACTIVE`</td>
      <td>%</td>
      <td>`namespace=${namespace}, pod=${pod}, node=${onodeName}` <br />
      `[GPU ${gpu} / ${GPU_I_ID}(${GPU_I_PROFILE}) / ${DCGM_FI_DEV_UUID}]`<br /> <br />
      e.g) <br />
      `namespace=whatap, pod=gpu-mig-5-a, node=ip-10-143-180 [GPU 5 / 11(2g.10gb) / MIG-1234-5678-c]` <br />
      `namespace=whatap, pod=gpu-no-mig-2, node=ip-10-143-140 [GPU 2 / null(null) / GPU-bcde-fghi]`</td>
      <td>The DCGM_FI_PROF_PIPE_TENSOR_ACTIVE metric collected from DCGM is adjusted by the weight (Compute Instance ratio) in MIG mode and aggregated per Pod unit.</td>
      <td><a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-ids.html">1004</a></td>
      <td>DCP</td>
    </tr>
    <tr>
      <td>`DCGM_FI_PROF_SM_OCCUPANCY for Devices`</td>
      <td>`DCGM_FI_PROF_SM_OCCUPANCY`</td>
      <td>%</td>
      <td>`GPU ${gpu} [Node: ${onodeName}]` <br /> e.g) `GPU 5 [Node: ip-10-143-180]`</td>
      <td> </td>
      <td><a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-ids.html">1003</a></td>
      <td>DCP</td>
    </tr>
    <tr>
      <td>`DCGM_FI_PROF_SM_OCCUPANCY for MIG Instance`</td>
      <td>`DCGM_FI_PROF_SM_OCCUPANCY`</td>
      <td>%</td>
      <td>`GPU ${gpu} / ${GPU_I_ID}(${GPU_I_PROFILE}) / ${DCGM_FI_DEV_UUID} [${onodeName}]` <br />
      e.g) `GPU 5 / 11(2g.10gb) / MIG-1234-5678-c [ip-10-143-180]`</td>
      <td> </td>
      <td><a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-ids.html">1003</a></td>
      <td>DCP</td>
    </tr>
    <tr>
      <td>`DCGM_FI_PROF_SM_OCCUPANCY Node-Level Average`</td>
      <td>`DCGM_FI_PROF_SM_OCCUPANCY`</td>
      <td>%</td>
      <td>`${onodeName}` <br /> e.g) `ip-10-143-180`</td>
      <td> </td>
      <td><a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-ids.html">1003</a></td>
      <td>DCP</td>
    </tr>
    <tr>
      <td>`DCGM_FI_PROF_SM_OCCUPANCY Pod-Level`</td>
      <td>`DCGM_FI_PROF_SM_OCCUPANCY`</td>
      <td>%</td>
      <td>`namespace=${namespace}, pod=${pod}, node=${onodeName}` <br />
      `[GPU ${gpu} / ${GPU_I_ID}(${GPU_I_PROFILE}) / ${DCGM_FI_DEV_UUID}]` <br /> <br />
      e.g)<br />
      `namespace=whatap, pod=gpu-mig-5-a, node=ip-10-143-180 [GPU 5 / 11(2g.10gb) / MIG-1234-5678-c]`<br />
      `namespace=whatap, pod=gpu-no-mig-2, node=ip-10-143-140 [GPU 2 / null(null) / GPU-bcde-fghi]`</td>
      <td> </td>
      <td><a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-ids.html">1003</a></td>
      <td>DCP</td>
    </tr>
    <tr>
      <td>`DCGM_FI_DEV_GPU_UTIL for Devices`</td>
      <td>`DCGM_FI_DEV_GPU_UTIL`</td>
      <td>%</td>
      <td>`GPU ${gpu} / [${onodeName}]` <br /> e.g) `GPU 2 / [10.143.140]`</td>
      <td>The DCGM_FI_DEV_GPU_UTIL metrics collected from DCGM are aggregated by GPU physical device unit.<br /> 
      (If the device is in MIG mode, this metric is not measured, and it is recommended to measure the GPU utilization via the DCGM_FI_PROF_GR_ENGINE_ACTIVE metric.)</td>
      <td><a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-ids.html">203</a></td>
      <td>Utilization</td>
    </tr>
    <tr>
      <td>`DCGM_FI_DEV_GPU_UTIL Node-Level Average`</td>
      <td>`DCGM_FI_DEV_GPU_UTIL`</td>
      <td>%</td>
      <td>`${onodeName}` <br /> e.g) `10.143.140`</td>
      <td>The DCGM_FI_DEV_GPU_UTIL metrics collected from DCGM are averaged by node to display the average GPU utilization of all nodes.</td>
      <td><a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-ids.html">203</a></td>
      <td>Utilization</td>
    </tr>
    <tr>
      <td>`DCGM_FI_DEV_GPU_UTIL Pod-Level`</td>
      <td>`DCGM_FI_DEV_GPU_UTIL`</td>
      <td>%</td>
      <td>`namespace=${namespace}, pod=${pod}, node=${onodeName} [GPU ${gpu} /` <br /> 
      `${uuid} ]e.g)namespace=whatap, pod=gpu-no-mig-4, node=ip-10-143-180 [GPU 4/ GPU-defg-`<br />
      `hijk]namespace=gpu, pod=gpu-no-mig-4, node=ip-10-143-180 [GPU 6/ GPU-efgh-ijkl]`</td>
      <td>The DCGM_FI_DEV_GPU_UTIL metrics collected from DCGM are averaged by Pod to display the GPU utilization for each Pod.</td>
      <td><a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-ids.html">203</a></td>
      <td>Utilization</td>
    </tr>
    <tr>
      <td>`DCGM_FI_DEV_POWER_USAGE for Devices`</td>
      <td>`DCGM_FI_DEV_POWER_USAGE`</td>
      <td>W</td>
      <td>`GPU ${gpu} [Node: ${onodeName}]` <br /> e.g) `GPU 5 [Node: ip-10-143-180]`</td>
      <td>The GPU power usage (Watt) metrics collected from DCGM measure the current power utilization by GPU physical device unit.</td>
      <td><a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-ids.html">155</a></td>
      <td>Power</td>
    </tr>
    <tr>
      <td>`DCGM_FI_DEV_POWER_USAGE for MIG Instance`</td>
      <td>`DCGM_FI_DEV_POWER_USAGE`</td>
      <td>W</td>
      <td>`GPU ${gpu} / ${GPU_I_ID}(${GPU_I_PROFILE}) / ${DCGM_FI_DEV_UUID} [${onodeName}]` <br /> 
      e.g) `GPU 5 / 11(2g.10gb) / MIG-1234-5678-c [ip-10-143-180]`</td>
      <td>The GPU power usage metrics collected from DCGM are measured by MIG instance unit.</td>
      <td><a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-ids.html">155</a></td>
      <td>Power</td>
    </tr>
    <tr>
      <td>`DCGM_FI_DEV_POWER_USAGE Node-Level Average`</td>
      <td>`DCGM_FI_DEV_POWER_USAGE`</td>
      <td>W</td>
      <td>`${onodeName}` <br /> e.g) `ip-10-143-180`</td>
      <td>The GPU power usage metrics collected from DCGM are measured by node unit.</td>
      <td><a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-ids.html">155</a></td>
      <td>Power</td>
    </tr>
    <tr>
      <td>`DCGM_FI_DEV_POWER_USAGE Pod-Level`</td>
      <td>`DCGM_FI_DEV_POWER_USAGE`</td>
      <td>W</td>
      <td>• mig mode=1 <br /> 
      `namespace=whatap, pod=gpu-mig-5-a, node=ip-10-143-180 [GPU 5 / 11(2g.10gb) / MIG-1234-5678-c]`<br />
      • mig mode=0<br />
      `namespace=whatap, pod=gpu-no-mig-2, node=ip-10-143-140 [GPU 2 / null(null) / GPU-bcde-fghi]`</td>
      <td>The GPU power usage metrics collected from DCGM are measured by Pod unit.</td>
      <td><a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-ids.html">155</a></td>
      <td>Power</td>
    </tr>
    <tr>
      <td>`DCGM_FI_DEV_FB_USED for Devices`</td>
      <td>`DCGM_FI_DEV_FB_USED`</td>
      <td>MiB</td>
      <td>`GPU ${gpu} [Node: ${onodeName}]` <br />e.g) `GPU 2 [Node: ip-10-143-140]`</td>
      <td>The DCGM_FI_DEV_FB_USED metrics collected from DCGM are aggregated by GPU physical device unit.</td>
      <td><a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-ids.html">252</a></td>
      <td>Framebuffer</td>
    </tr>
    <tr>
      <td>`DCGM_FI_DEV_FB_USED for MIG Instance`</td>
      <td>`DCGM_FI_DEV_FB_USED`</td>
      <td>MiB</td>
      <td>`GPU ${gpu} / ${GPU_I_ID}(${GPU_I_PROFILE}) / ${DCGM_FI_DEV_UUID} [${onodeName}]` <br />
      e.g) `GPU 5 / 11(2g.10gb) / MIG-1234-5678-c [ip-10-143-180]`</td>
      <td>The DCGM_FI_DEV_FB_USED metrics collected from DCGM are aggregated by MIG instance unit.</td>
      <td><a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-ids.html">252</a></td>
      <td>Framebuffer</td>
    </tr>
    <tr>
      <td>`DCGM_FI_DEV_FB_USED Node-Level Average`</td>
      <td>`DCGM_FI_DEV_FB_USED`</td>
      <td>MiB</td>
      <td>`${onodeName}` <br /> e.g) `ip-10-143-180`</td>
      <td>The DCGM_FI_DEV_FB_USED metrics collected from DCGM are aggregated by node unit.</td>
      <td><a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-ids.html">252</a></td>
      <td>Framebuffer</td>
    </tr>
    <tr>
      <td>`DCGM_FI_DEV_FB_USED Pod-Level`</td>
      <td>`DCGM_FI_DEV_FB_USED`</td>
      <td>MiB</td>
      <td>`namespace=${namespace}, pod=${pod}, node=${onodeName}`<br /> `[GPU ${gpu} / ${uuid}]`<br />
      e.g) <br /> `namespace=whatap, pod=gpu-no-mig-4, node=ip-10-143-180 [GPU 4/ GPU-defg-hijk]`</td>
      <td>The DCGM_FI_DEV_FB_USED metrics collected from DCGM are aggregated by Pod unit.</td>
      <td><a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-ids.html">252</a></td>
      <td>Framebuffer</td>
    </tr>
    <tr>
      <td>`DCGM_FI_DEV_FB_USED_PERCENT for Devices`</td>
      <td>`DCGM_FI_DEV_FB_USED_PERCENT`</td>
      <td>%</td>
      <td>`GPU ${gpu} [Node: ${onodeName}]` <br /> e.g) `GPU 5 [Node: ip-10-143-180]`</td>
      <td>The DCGM_FI_DEV_FB_PERCENT metrics collected from DCGM are calculated by GPU physical device unit.</td>
      <td><a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-ids.html">254</a></td>
      <td>Framebuffer</td>
    </tr>
    <tr>
      <td>`DCGM_FI_DEV_FB_USED_PERCENT for MIG Instance`</td>
      <td>`DCGM_FI_DEV_FB_USED_PERCENT`</td>
      <td>%</td>
      <td>`GPU ${gpu} / ${GPU_I_ID}(${GPU_I_PROFILE}) / ${DCGM_FI_DEV_UUID} [${onodeName}]` <br /> e.g) `GPU 5 / 11(2g.10gb) / MIG-1234-5678-c [ip-10-143-180]`</td>
      <td>The DCGM_FI_DEV_FB_PERCENT metrics collected from DCGM are calculated by MIG instance unit.</td>
      <td><a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-ids.html">254</a></td>
      <td>Framebuffer</td>
    </tr>
    <tr>
      <td>`DCGM_FI_DEV_FB_USED_PERCENT Node-Level Average`</td>
      <td>`DCGM_FI_DEV_FB_USED_PERCENT`</td>
      <td>%</td>
      <td>`${onodeName}` <br /> e.g) `ip-10-143-180`</td>
      <td>The DCGM_FI_DEV_FB_PERCENT metrics collected from DCGM are averaged by node unit.</td>
      <td><a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-ids.html">254</a></td>
      <td>Framebuffer</td>
    </tr>
    <tr>
      <td>`DCGM_FI_DEV_FB_USED_PERCENT Pod-Level`</td>
      <td>`DCGM_FI_DEV_FB_USED_PERCENT`</td>
      <td>%</td>
      <td>• mig mode=1 <br />
      `namespace=whatap, pod=gpu-mig-5-a, node=ip-10-143-180 [GPU 5 / 11(2g.10gb) / MIG-1234-5678-c]`<br />
      • mig mode=0 <br /> 
      `namespace=whatap, pod=gpu-no-mig-2, node=ip-10-143-140 [GPU 2 / null(null) / GPU-bcde-fghi]`</td>
      <td>The DCGM_FI_DEV_FB_PERCENT metrics collected from DCGM are calculated by Pod unit.</td>
      <td><a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-ids.html">254</a></td>
      <td>Framebuffer</td>
    </tr>
    <tr>
      <td>`DCGM_FI_DEV_GPU_TEMP for Devices`</td>
      <td>`DCGM_FI_DEV_GPU_TEMP`</td>
      <td>°C</td>
      <td>`GPU ${gpu} / [${onodeName}]` <br /> e.g) `GPU 2 / [10.143.140]`</td>
      <td>The DCGM_FI_DEV_GPU_TEMP metrics collected from DCGM are aggregated by GPU physical device unit.</td>
      <td><a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-ids.html">150</a></td>
      <td>Temperature</td>
    </tr>
    <tr>
      <td>`DCGM_FI_DEV_GPU_TEMP Node-Level Average`</td>
      <td>`DCGM_FI_DEV_GPU_TEMP`</td>
      <td>°C</td>
      <td>`${onodeName}` <br /> e.g) `ip-10-143-180`</td>
      <td>The DCGM_FI_DEV_GPU_TEMP metrics collected from DCGM are averaged by node unit.</td>
      <td><a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-ids.html">150</a></td>
      <td>Temperature</td>
    </tr>
    <tr>
      <td>`DCGM_FI_DEV_MEM_COPY_UTIL for Devices`</td>
      <td>`DCGM_FI_DEV_MEM_COPY_UTIL`</td>
      <td>%</td>
      <td>`GPU ${gpu} / [${onodeName}]`<br /> e.g) `GPU 2 / [10.143.140]`</td>
      <td>The DCGM_FI_DEV_MEM_COPY_UTIL metrics collected from DCGM are aggregated by GPU physical device unit.</td>
      <td><a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-ids.html">204</a></td>
      <td>Utilization</td>
    </tr>
    <tr>
      <td>`DCGM_FI_DEV_MEM_COPY_UTIL Node-Level Average`</td>
      <td>`DCGM_FI_DEV_MEM_COPY_UTIL`</td>
      <td>%</td>
      <td>`${onodeName}` <br /> e.g) `ip-10-143-180`</td>
      <td>The DCGM_FI_DEV_MEM_COPY_UTIL metrics collected from DCGM are averaged by node unit.</td>
      <td><a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-ids.html">204</a></td>
      <td>Utilization</td>
    </tr>
    <tr>
      <td>`DCGM_FI_DEV_MEM_COPY_UTIL Pod-Level`</td>
      <td>`DCGM_FI_DEV_MEM_COPY_UTIL`</td>
      <td>%</td>
      <td>`namespace=${namespace}, pod=${pod}, node=${onodeName}`<br />
      `[GPU ${gpu} / ${uuid}]` <br />
      e.g) <br />`namespace=whatap, pod=gpu-no-mig-4, node=ip-10-143-180 [GPU 4/ GPU-defg-hijk]`</td>
      <td>The DCGM_FI_DEV_MEM_COPY_UTIL metrics collected from DCGM are calculated by Pod unit.</td>
      <td><a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-ids.html">204</a></td>
      <td>Utilization</td>
    </tr>
    <tr>
      <td>`DCGM_FI_DEV_SM_CLOCK for Devices`</td>
      <td>`DCGM_FI_DEV_SM_CLOCK`</td>
      <td>MHz</td>
      <td>`GPU ${gpu} [Node: ${onodeName}]` <br /> e.g) `GPU 5 [Node: ip-10-143-180]`</td>
      <td>The SM clocks (MHz) collected from DCGM are averaged and aggregated by GPU physical device unit.</td>
      <td><a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-ids.html">100</a></td>
      <td>Clocks</td>
    </tr>
    <tr>
      <td>`DCGM_FI_DEV_SM_CLOCK for MIG Instance`</td>
      <td>`DCGM_FI_DEV_SM_CLOCK`</td>
      <td>MHz</td>
      <td>`GPU ${gpu} / ${GPU_I_ID}(${GPU_I_PROFILE}) / ${DCGM_FI_DEV_UUID} [${onodeName}]` <br />
      e.g) `GPU 5 / 11(2g.10gb) / MIG-1234-5678-c [ip-10-143-180]`</td>
      <td>The SM clocks collected from DCGM are averaged and aggregated by MIG instance unit.</td>
      <td><a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-ids.html">100</a></td>
      <td>Clocks</td>
    </tr>
    <tr>
      <td>`DCGM_FI_DEV_SM_CLOCK Node-Level Average`</td>
      <td>`DCGM_FI_DEV_SM_CLOCK`</td>
      <td>MHz</td>
      <td>`${onodeName}` <br /> e.g) `ip-10-143-180`</td>
      <td>The SM clocks collected from DCGM are averaged and aggregated by node unit.</td>
      <td><a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-ids.html">100</a></td>
      <td>Clocks</td>
    </tr>
    <tr>
      <td>`DCGM_FI_DEV_SM_CLOCK for Pod-Level`</td>
      <td>`DCGM_FI_DEV_SM_CLOCK`</td>
      <td>MHz</td>
      <td>`namespace=${namespace}, pod=${pod}, node=${onodeName}` <br />
      `[GPU ${gpu} / ${GPU_I_ID}(${GPU_I_PROFILE}) / ${DCGM_FI_DEV_UUID}]` <br />
      e.g)<br /> `namespace=whatap, pod=gpu-mig-5-a, node=ip-10-143-180 [GPU 5 / 11(2g.10gb) / MIG-1234-5678-c]` <br />
      `namespace=whatap, pod=gpu-no-mig-2, node=ip-10-143-140 [GPU 2 / null(null) / GPU-bcde-fghi]`</td>
      <td>The SM clocks collected from DCGM are aggregated by Pod unit.</td>
      <td><a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-ids.html">100</a></td>
      <td>Clocks</td>
    </tr>
    <tr>
      <td>`DCGM_FI_DEV_MEM_CLOCK for Devices`</td>
      <td>`DCGM_FI_DEV_MEM_CLOCK`</td>
      <td>MHz</td>
      <td>`GPU ${gpu} [Node: ${onodeName}]` <br /> e.g) `GPU 5 [Node: ip-10-143-180]`</td>
      <td>The DCGM_FI_DEV_MEM_CLOCK metrics collected from DCGM are aggregated by GPU physical device unit.</td>
      <td><a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-ids.html">101</a></td>
      <td>Clock</td>
    </tr>
    <tr>
      <td>`DCGM_FI_DEV_MEM_CLOCK for MIG Instance`</td>
      <td>`DCGM_FI_DEV_MEM_CLOCK`</td>
      <td>MHz</td>
      <td>`GPU ${gpu} / ${GPU_I_ID}(${GPU_I_PROFILE}) / ${DCGM_FI_DEV_UUID} [${onodeName}]` <br />
      e.g) `GPU 5 / 11(2g.10gb) / MIG-1234-5678-c [ip-10-143-180]`</td>
      <td>The DCGM_FI_DEV_MEM_CLOCK metrics collected from DCGM are aggregated by MIG instance unit.</td>
      <td><a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-ids.html">101</a></td>
      <td>Clock</td>
    </tr>
    <tr>
      <td>`DCGM_FI_DEV_MEM_CLOCK Node-Level Average`</td>
      <td>`DCGM_FI_DEV_MEM_CLOCK`</td>
      <td>MHz</td>
      <td>`${onodeName}` <br /> e.g) `ip-10-143-180`</td>
      <td>The DCGM_FI_DEV_MEM_CLOCK metrics collected from DCGM are aggregated by node unit.</td>
      <td><a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-ids.html">101</a></td>
      <td>Clock</td>
    </tr>
    <tr>
      <td>`Pod-Level DCGM_FI_DEV_MEM_CLOCK for Pod-Level `</td>
      <td>`DCGM_FI_DEV_MEM_CLOCK`</td>
      <td>MHz</td>
      <td>• mig mode=1 <br />
      `namespace=whatap, pod=gpu-mig-5-a, node=ip-10-143-180 [GPU 5 / 11(2g.10gb) / MIG-1234-5678-c]` <br />
      • mig mode=0 <br />
      `namespace=whatap, pod=gpu-no-mig-2, node=ip-10-143-140 [GPU 2 / null(null) / GPU-bcde-fghi]`</td>
      <td>The DCGM_FI_DEV_MEM_CLOCK metrics collected from DCGM are aggregated by Pod unit.</td>
      <td><a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-ids.html">101</a></td>
      <td>Clock</td>
    </tr>
    <tr>
      <td>`DCGM_FI_DEV_PSTATE for Devices`</td>
      <td>`DCGM_FI_DEV_PSTATE`</td>
      <td>INT</td>
      <td>`GPU ${gpu} [Node: ${onodeName}]` <br /> e.g) `GPU 5 [Node: ip-10-143-180]`</td>
      <td>The DCGM_FI_DEV_PSTATE metrics collected from DCGM are measured by GPU physical device unit with the MIG mode disabled.</td>
      <td><a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-ids.html">190</a></td>
      <td>STATUS</td>
    </tr>
    <tr>
      <td>`DCGM_FI_DEV_PSTATE for MIG Instance`</td>
      <td>`DCGM_FI_DEV_PSTATE`</td>
      <td>INT</td>
      <td>`GPU ${gpu} / ${GPU_I_ID}($GPU_I_PROFILE) / ${DCGM_FI_DEV_UUID} [${onodeName}]` <br /> 
      e.g) `GPU 5 / 11(2g.10gb) / MIG-1234-5678-c [ip-10-143-180]`</td>
      <td>The DCGM_FI_DEV_PSTATE metrics collected from DCGM are measured by MIG instance unit in MIG mode.</td>
      <td><a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/dcgm-api-field-ids.html">190</a></td>
      <td>STATUS</td>
    </tr>
  </tbody>
</table>
</div>
